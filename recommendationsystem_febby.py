# -*- coding: utf-8 -*-
"""RecommendationSystem_Febby.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11l53g211AwZKASpbROz8L88-cz7PMoW5

## SYSTEM RECOMMENDATION - Proyek 2
* Nama : Febby Ariyanti Herdiana

## Importing Library
"""

import pandas as pd
import numpy as np
import zipfile
import glob
import os
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import keras

from zipfile import ZipFile
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d arashnic/book-recommendation-dataset

"""## Ekstrak Data"""

local_zip = '/content/book-recommendation-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""## Data Loading"""

books = pd.read_csv('/content/Books.csv')
ratings = pd.read_csv('/content/Ratings.csv')

books

ratings

# melihat ukuran data
print(books.shape)
print(ratings.shape)

"""Berdasarkan data di atas dataset ini berisi :

* Data books terdapat 271360 baris dan 8 kolom variabel
* Data ratings terdapat 1149780 baris dan 3 kolom variabel

## Exploratory Data Analysis (EDA)

# Deskripsi Variabel

Books.csv:

* ISBN : Nomor Buku Standar Internasional (International Standard Book Number)
* Book-Title : Judul buku
* Book-Author : Penulis buku
* Year-Of-Publication : Tahun buku diterbitkan
* Publisher : Penerbit buku
* Image-URL-S : URL sampul buku berukuran kecil
* Image-URL-M : URL gambar sampul buku berukuran sedang
* Image-URL-L : URL gambar sampul buku berukuran besar

Ratings.csv:

* User-ID : ID/kode unik bagi pengguna
* ISBN : Nomor Buku Standar Internasional (International Standard Book Number)
* Book-Rating : Rating buku dari user
"""

print('Jumlah data buku: ', len(books['ISBN'].unique()))
print('Jumlah data user yang memberikan rating: ', len(ratings['User-ID'].unique()))
print('Jumlah data rating pada buku: ', len(ratings['ISBN'].unique()))

"""## Univariate Data Analysis"""

books.info()

"""Berdasarkan data di atas, dataframe books memiliki 271360 data entri dan terdapat 8 variabel yang semua bertipe data object.

Mengecek berapa banyak data entri yang unik dari masing-masing variabel dan merename kolom header.
"""

books = books.rename(columns={'Book-Title': 'book_title','Book-Author':'book_author',
                              'Year-Of-Publication':'year_of_publication','Image-URL-S':'Image_URL_S',
                              'Image-URL-M':'Image_URL_M','Image-URL-L':'Image_URL_L'})
books.head(4)

book_list = books['book_title'].value_counts().keys()
jumlah = books['book_title'].value_counts()

book_count = pd.DataFrame({'book_title': book_list, 'Jumlah': jumlah}).reset_index(drop=True)
book_count

# mengecek entri unik dr variabel Book-Title
print('Banyak Data : ', len(books.book_title.unique()))
print('Judul Buku : ', books.book_title.unique())

author_list = books['book_author'].value_counts().keys()
jumlah = books['book_author'].value_counts()

author_count = pd.DataFrame({'book_author': author_list, 'Jumlah': jumlah}).reset_index(drop=True)
author_count

# melihat distribusi data variabel Year of Publication
import matplotlib.pyplot as plt
count = books["year_of_publication"].value_counts()
count.plot(kind='bar', title="Year of Publication");
 
plt.show()

"""Dari grafik gambar di atas terlihat bahwa distribusi data Year of Publication cenderung berjenis right-skewed."""

ratings.info()

rating_list = ratings['Book-Rating'].value_counts().keys()
jumlah = ratings['Book-Rating'].value_counts()

rating_count = pd.DataFrame({'Ratings': rating_list, 'Jumlah': jumlah}).reset_index(drop=True)
rating_count

sns.barplot(data=rating_count, x='Ratings', y='Jumlah')
plt.show()

"""Dari visualisasi di atas, diketahui bahwa nilai maksimum rating adalah 10 dan nilai minimumnya adalah 0. Artinya, skala rating berkisar antara 0 hingga 10.

## Data Pre-Processing

Penggabungan Data Buku dan Data Rating
"""

# Menggabungkan dataframe books dengan ratings dan memasukkannya ke dalam variabel ds
db = pd.merge(books, ratings, on='ISBN', how='left')
db

"""Karena jumlah data di atas mencapai 1032345 maka data hanya akan diambil menjadi 100000 sample data."""

db = db[:100000]
db

"""## Data Preparation

### Memeriksa dan Menangani Missing Value
"""

# mengecek missing value pada dataframe
db.isnull().sum()

# membersihkan missing value dengan fungsi dropna()
db_clean = db.dropna()
db_clean

# Mengecek kembali missing value pada variabel ds_clean
db_clean.isnull().sum()

"""#### Memeriksa dan Menangani Duplikasi Data"""

for col in books.columns:
  print(f'{col}: {books[col].duplicated().sum()}')

# Membuat variabel preparation yang berisi dataframe ds_clean kemudian mengurutkan berdasarkan book_title
preparation = db_clean
preparation.sort_values('book_title')

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('book_title')
preparation

"""### Mengubah Dataframe menjadi List"""

book_ISBN = preparation['ISBN'].tolist()
title = preparation['book_title'].tolist()
author = preparation['book_author'].tolist()
book_year_of_publication = preparation['year_of_publication'].tolist()
book_publisher = preparation['Publisher'].tolist()

print(len(book_ISBN))
print(len(title))
print(len(author))
print(len(book_year_of_publication))
print(len(book_publisher))

"""Membuat dictionary untuk menentukan pasangan key-value"""

db_new = pd.DataFrame({
    'book_ISBN': book_ISBN,
    'title': title,
    'author': author,
    'book_year_of_publication': book_year_of_publication,
    'book_publisher': book_publisher
})
db_new

"""## Model Content Based Filtering

Pengembangan model menggunakan pendekatan Content-Based Filtering ini dilakukan untuk menghasilkan rekomendasi buku berdasarkan nama penulis buku yang pernah dibaca oleh pengguna (user).
"""

data = db_new
data.sample(4)

"""## TF-IDF Vectorizer

* Pada pemodelan dengan Content-Based Filtering ini, teknik TF-IDF Vectorizer akan digunakan pada sistem rekomendasi untuk menemukan representasi fitur penting dari setiap nama penulis buku (author).

* TF-IDF atau Term Frequency-Inverse Document Frequency berfungsi untuk mengukur seberapa penting suatu kata terhadap kata-kata lain yang ada dalam dokumen.

Pada kode berikut kita akan mengambil kata-kata penting dari variabel author untuk mendapatkan rekomendasi berdasarkan nama penulis.
"""

tfid = TfidfVectorizer()
tfid.fit(data['author']) 

tfid.get_feature_names()

tfidf_matrix = tfid.fit_transform(data['author']) 
tfidf_matrix.shape

tfidf_matrix.todense()

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## Mendapatkan Rekomendasi Buku"""

def author_recommendations(title, similarity_data=cosine_sim_df, items=data[['title', 'author']], k=5):
  index = similarity_data.loc[:, title].to_numpy().argpartition(range(-1, -k, -1))
  closest = similarity_data.columns[index[-1:-(k+2):-1]]
  closest = closest.drop(title, errors='ignore')
  return pd.DataFrame(closest).merge(items).head(k)

data.sample(5)

# mengecek berdasarkan judul buku
data[data.title.eq('Sudden Prey')]

# Mendapatkan rekomendasi judul buku berdasarkan nama penulis (author) dari buku yang berjudul Deception Point
author_recommendations('Sudden Prey')

"""## Model Collaborative Filtering

Tidak seperti pada teknik Content-Based Filtering. Data yang digunakan di teknik Collaborative Filtering kali ini tidak memerlukan data Book-Author, dan Num-Ratings. Sebab pada teknik ini hanya menggunakan rating sebagai acuan sistem rekomendasi.
"""

# read dataset
df = ratings
df

"""### Data Preparation

### Menyandikan Fitur

Pada tahap ini, kita perlu melakukan persiapan data untuk menyandikan (encode) fitur 'user_id' dan 'ISBN' ke dalam indeks integer. Berikut penerapannya.
"""

user_ids = df['User-ID'].unique().tolist()
user2encoded = {x: i for i, x in enumerate(user_ids)}
encoded2user = {i: x for i, x in enumerate(user_ids)}

book_isbns = df['ISBN'].unique().tolist()
book2encoded = {x: i for i, x in enumerate(book_isbns)}
encoded2book = {i: x for i, x in enumerate(book_isbns)}

df['User-Encoded'] = df['User-ID'].map(user2encoded)
df['Book-Encoded'] = df['ISBN'].map(book2encoded)

num_users = len(user2encoded)
print(num_users)
 
num_books = len(encoded2book)
print(num_books)

df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)
 
min_rating = min(df['Book-Rating'])
max_rating = max(df['Book-Rating'])

print(f'Number of User: {num_users}, Number of Books: {num_books}, Min Rating: {min_rating}, Max Rating: {max_rating}')

df

"""## Normalisasi Data Rating

Melakukan transformasi pada data fitur Book-Rating. MinMaxScaler mentransformasikan fitur dengan menskalakan setiap fitur ke rentang tertentu. Library ini menskalakan dan mentransformasikan setiap fitur secara individual sehingga berada dalam rentang yang diberikan pada set pelatihan, pada library ini memiliki range default antara nol dan satu.
"""

x = df[['User-Encoded', 'Book-Encoded']].values
y = df['Book-Rating'].values
y = y.reshape(-1, 1)

scaler = MinMaxScaler()
norm_y = scaler.fit_transform(y)
norm_y = norm_y.reshape(1, -1)[0]

"""## Split Dataset"""

x_train, x_val, y_train, y_val = train_test_split(x, norm_y, test_size=0.1, random_state=123)

def create_dataset(x, y, batch_size, buffer_size=None, shuffle=True):
  ds = tf.data.Dataset.from_tensor_slices((x, y))

  if shuffle:
    ds = ds.shuffle(buffer_size)

  ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)

  return ds

batch_size = 128
buffer_size = len(x)

train_ds = create_dataset(x_train, y_train, batch_size, buffer_size)
val_ds = create_dataset(x_val, y_val, batch_size, shuffle=False)

"""## Modelling"""

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)

    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-3),
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.books_embedding = layers.Embedding(
        num_books,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-3),
    )
    self.books_bias = layers.Embedding(num_books, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:, 0])
    user_bias = self.user_bias(inputs[:, 0])
    books_vector = self.books_embedding(inputs[:, 1])
    books_bias = self.books_bias(inputs[:, 1])

    dot_user_books = tf.tensordot(user_vector, books_vector, 2)

    x = dot_user_books + user_bias + books_bias

    return tf.nn.sigmoid(x)

embedding_size = 32

model = RecommenderNet(num_users, num_books, embedding_size)
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""## Model Training"""

history = model.fit(
  train_ds,
  epochs = 20,
  validation_data = val_ds,
  verbose=1,
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['root_mean_squared_error', 'val_root_mean_squared_error'])
plt.show()

"""## Mendapatkan Rekomendasi Buku"""

book_df = db_new
df = pd.read_csv('/content/Ratings.csv')

# rename header kolom
df = df.rename(columns={'User-ID': 'user_id','Book-Rating':'book_rating'})
#ratings.head()
 
# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
book_read_by_user = df[df.user_id == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
book_never_read = book_df[~book_df['book_ISBN'].isin(book_read_by_user.ISBN.values)]['book_ISBN'] 
book_never_read = list(
    set(book_never_read)
    .intersection(set(book2encoded.keys()))
)
 
book_never_read = [[book2encoded.get(x)] for x in book_never_read]
user_encoder = user2encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_never_read), book_never_read)
)

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    encoded2book.get(book_never_read[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)
 
top_book_user = (
    book_read_by_user.sort_values(
        by = 'book_rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['book_ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.title, ':', row.author)
 
print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)
 
recommended_book = book_df[book_df['book_ISBN'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.title, ':', row.author)

"""## Evaluation

1. Content Based Filtering

Untuk evaluasi model dengan pendekatan ini akan menggunakan metrik precision. Berikut adalah formulanya :

Precision = #of recommendation that are relevant / #of item we recommend 

Metrik precision akan menghitung jumlah item rekomendasi yang relevan dari keseluruhan rekomendasi. Relevansi ditentukan dengan banyaknya jumlah rekomendasi yang sesuai/mirip (similar) dengan preferensi pengguna (dalam hal ini sesuai dengan nama penulis buku yang pernah dibaca oleh pengguna).

Melihat dari hasil rekomendasi yang diberikan, sistem telah memberikan rekomendasi buku berdasarkan kata kunci nama penulis (author) dari buku yang pernah dibaca oleh pengguna (user). Dalam hal ini terdapat 2 buku yang relevan dengan preferensi pengguna, maka :

Precision = 2/5
Precision = 40%

Hal ini dapat diartikan bahwa sistem rekomendasi yang dibuat memiliki presisi 40% dan sudah bisa memberikan rekomendasi sesuai dengan tujuan dari pengembangan sistem, yakni untuk menghasilkan rekomendasi buku berdasarkan nama penulis buku yang pernah dibaca oleh pengguna (user).

2. Collaborative Filtering

Dari hasil plot metrik, dapat diartikan bahwa :

Proses training model untuk data train cukup smooth walaupun pada epoch awal nilai error sempat naik, tapi kemudian menurun secara signifikan hingga nilai error akhir sebesar 0.2837.
Sedangkan pada data validasi, nilai error tidak menurun secara signifikan, dan error akhirnya sebesar 0.5234.
Walaupun nilai error yang didapat cukup baik untuk sistem rekomendasi, namun model ini masih underfitting.

## References

[1] Romadhon, A. C. (2020). Pentingnya Membaca Dan Menulis Serta Kaitannya Dengan Kemajuan Peradaban Bangsa.

[2] Ilham, Bahrul U. (2022). Harbuknas 2022: Literasi Indonesia Peringkat Ke-62 Dari 70 Negara. Retrieved [16 Oktober 2022] from : [Link](https://bisniskumkm.com/harbuknas-2022-literasi-indonesia-peringkat-ke-62-dari-70-negara/#:~:text=Harbuknas%202022%20%3A%20Literasi%20Indonesia%20Peringkat%20Ke%2D62%20Dari%2070%20negara,-UNESCO%20Menyebut%20indeks)

[3] Book Recommendation Dataset (https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset) : Collected by Cai-Nicolas Ziegler in a 4-week crawl (August / September 2004) from the Book-Crossing community with kind permission from Ron Hornbaker, CTO of Humankind Systems. Contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit / implicit) about 271,379 books.

[4] Rhys, Hefin. "Machine Learning with R, the Tidyverse, and MLR". Manning Publications. 2020. Page 286. Tersedia: [O'Reilly Media](https://learning.oreilly.com/library/view/machine-learning-with/9781617296574/).
"""